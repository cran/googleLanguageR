---
title: "Google Cloud Speech API"
author: "Mark Edmondson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Google Cloud Speech API}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The Google Cloud Speech API enables you to convert audio to text by applying neural network models in an easy to use API. The API recognizes over 80 languages and variants, to support your global user base. You can transcribe the text of users dictating to an application’s microphone or enable command-and-control through voice among many other use cases. 

Read more [on the Google Cloud Speech Website](https://cloud.google.com/speech/)

The Cloud Speech API provides audio transcription.  Its accessible via the `gl_speech` function.

Arguments include:

* `audio_source` - this is a local file in the correct format, or a Google Cloud Storage URI
* `encoding` - the format of the sound file - `LINEAR16` is the common `.wav` format, other formats include `FLAC` and `OGG_OPUS`
* `sampleRate` - this needs to be set to what your file is recorded at.  
* `languageCode` - specify the language spoken as a [`BCP-47` language tag](https://tools.ietf.org/html/bcp47)
* `speechContexts` - you can supply keywords to help the translation with some context. 

### Demo for Google Cloud Speech API

A test audio file is installed with the package which reads:

> "To administer medicine to animals is frequently a very difficult matter, and yet sometimes it's necessary to do so"

The file is sourced from the [University of Southampton's speech detection](http://www-mobile.ecs.soton.ac.uk/newcomms/) group and is fairly difficult for computers to parse, as we see below:

```r
library(googleLanguageR)
## get the sample source file
test_audio <- system.file("woman1_wb.wav", package = "googleLanguageR")

## its not perfect but...:)
gl_speech(test_audio)$transcript

## get alternative transcriptions
gl_speech(test_audio, maxAlternatives = 2L)$transcript

gl_speech(test_audio, languageCode = "en-GB")$transcript

## help it out with context for "frequently"
gl_speech(test_audio, 
            languageCode = "en-GB", 
            speechContexts = list(phrases = list("is frequently a very difficult")))$transcript
```

### Word transcripts

The API [supports timestamps](https://cloud.google.com/speech/reference/rest/v1/speech/recognize#WordInfo) on when words are recognised. These are outputted into a list column that holds three entries: `startTime`, `endTime` and the `word`.

To access this data, use `tidyr::unnest()` which will duplicate the other columns.

```r
> str(result)
#Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	1 obs. of  3 variables:
# $ transcript: chr "to administer medicine to animals Is frequent very difficult matter and yet #sometimes it's necessary to do so"
# $ confidence: num 0.929
# $ words     :List of 1
#  ..$ :'data.frame':	18 obs. of  3 variables:
#  .. ..$ startTime: chr  "0s" "0.200s" "0.700s" "1.100s" ...
#  .. ..$ endTime  : chr  "0.200s" "0.700s" "1.100s" "1.400s" ...
#  .. ..$ word     : chr  "to" "administer" "medicine" "to" ...

## extracting words
unnested <- tidyr::unnest(result)
str(unnested)
#Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	18 obs. of  5 variables:
# $ transcript: chr  "to administer medicine to animals Is frequent very difficult matter and yet #sometimes it's necessary to do so" "to administer medicine to animals Is frequent very difficult #matter and yet sometimes it's necessary to do so" "to administer medicine to animals Is frequent very #difficult matter and yet sometimes it's necessary to do so" "to administer medicine to animals Is #frequent very difficult matter and yet sometimes it's necessary to do so" ...
# $ confidence: num  0.929 0.929 0.929 0.929 0.929 ...
# $ startTime : chr  "0s" "0.200s" "0.700s" "1.100s" ...
# $ endTime   : chr  "0.200s" "0.700s" "1.100s" "1.400s" ...
# $ word      : chr  "to" "administer" "medicine" "to" ...
```

## Asynchronous calls

For speech files greater than 60 seconds of if you don't want your results straight away, set `asynch = TRUE` in the call to the API.

This will return an object of class `"gl_speech_op"` which should be used within the `gl_speech_op()` function to check the status of the task.  If the task is finished, then it will return an object the same form as the non-asynchronous case. 

```r
async <- gl_speech(test_audio, asynch = TRUE)
async
## Send to gl_speech_op() for status
## 4625920921526393240

result <- gl_speech_op(async)
> str(result)
#Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	1 obs. of  3 variables:
# $ transcript: chr "to administer medicine to animals Is frequent very difficult matter and yet #sometimes it's necessary to do so"
# $ confidence: num 0.929
# $ words     :List of 1
#  ..$ :'data.frame':	18 obs. of  3 variables:
#  .. ..$ startTime: chr  "0s" "0.200s" "0.700s" "1.100s" ...
#  .. ..$ endTime  : chr  "0.200s" "0.700s" "1.100s" "1.400s" ...
#  .. ..$ word     : chr  "to" "administer" "medicine" "to" ...
```

